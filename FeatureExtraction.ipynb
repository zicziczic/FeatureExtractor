{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0596a33a-c240-4f9a-ae77-cbe031a76d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import ViTModel, ViTImageProcessor, AutoModelForImageClassification, ViTImageProcessorFast\n",
    "from PIL import Image\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9f7ecce3-717d-4541-b3c0-2e8b7ac83ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models(modelo):\n",
    "\n",
    "    if modelo == 'ViT_huge':\n",
    "        vit_huge_model = ViTModel.from_pretrained('google/vit-huge-patch14-224-in21k')\n",
    "        vit_huge_feature_extractor = ViTImageProcessor.from_pretrained('google/vit-huge-patch14-224-in21k')\n",
    "        vit_huge_model.eval()\n",
    "\n",
    "        return vit_huge_model, vit_huge_feature_extractor\n",
    "\n",
    "    if modelo == 'ViT_large':\n",
    "        vit_large_model = ViTModel.from_pretrained('google/vit-large-patch16-224-in21k')\n",
    "        vit_large_feature_extractor = ViTImageProcessor.from_pretrained('google/vit-large-patch16-224-in21k')\n",
    "        vit_large_model.eval()\n",
    "\n",
    "        return vit_large_model, vit_large_feature_extractor\n",
    "    \n",
    "    if modelo == 'ViT_base':\n",
    "        vit_base_model = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "        vit_base_feature_extractor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "        vit_base_model.eval()\n",
    "    \n",
    "        return vit_base_model, vit_base_feature_extractor\n",
    "\n",
    "    if modelo == 'ViT_small':\n",
    "        vit_small_model = AutoModelForImageClassification.from_pretrained('WinKawaks/vit-small-patch16-224')\n",
    "        vit_small_feature_extractor = ViTImageProcessorFast.from_pretrained('WinKawaks/vit-small-patch16-224')\n",
    "        vit_small_model.eval()\n",
    "\n",
    "        return vit_small_model, vit_small_feature_extractor\n",
    "\n",
    "    return \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0ee20eb3-7760-4613-96d0-bd71599ceb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(image_path, modelo, feature_extractor):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = modelo(**inputs)\n",
    "    \n",
    "    features = outputs.logits.mean(dim=1).squeeze().numpy()\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "70901cee-bbd1-47cf-87d1-1d4cdb0ae9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_to_df(folder_path, modelo, feature_extractor):\n",
    "    data = []\n",
    "    \n",
    "    for subfolder in tqdm(os.listdir(folder_path)):\n",
    "        subfolder_path = os.path.join(folder_path, subfolder)\n",
    "    \n",
    "    if os.path.isdir(subfolder_path):\n",
    "        for image_file in os.listdir(subfolder_path):\n",
    "            image_path = os.path.join(subfolder_path, image_file)\n",
    "            \n",
    "            if image_file.lower().endswith(('png', 'jpg', 'jpeg')):\n",
    "                features = feature_extraction(image_path, modelo, feature_extractor)\n",
    "                    \n",
    "                data.append([image_path, *features])\n",
    "\n",
    "    columns = ['image_path'] + [f'feature_{i}' for i in range(len(features))]\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "    return df\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "79279662-cb86-414d-8dc6-e52188958181",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, fe = load_models('ViT_small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4b34423a-c6b0-4928-aed2-96b48fae6d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 5/5 [00:00<00:00, 4744.69it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "iteration over a 0-d array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mfeatures_to_df\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./Soybean Seeds/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfe\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[44], line 14\u001b[0m, in \u001b[0;36mfeatures_to_df\u001b[0;34m(folder_path, modelo, feature_extractor)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m image_file\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpng\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjpg\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjpeg\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n\u001b[1;32m     12\u001b[0m             features \u001b[38;5;241m=\u001b[39m feature_extraction(image_path, modelo, feature_extractor)\n\u001b[0;32m---> 14\u001b[0m             data\u001b[38;5;241m.\u001b[39mappend([image_path, \u001b[38;5;241m*\u001b[39mfeatures])\n\u001b[1;32m     16\u001b[0m columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_path\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(features))]\n\u001b[1;32m     17\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data, columns\u001b[38;5;241m=\u001b[39mcolumns)\n",
      "\u001b[0;31mTypeError\u001b[0m: iteration over a 0-d array"
     ]
    }
   ],
   "source": [
    "df = features_to_df('./Soybean Seeds/',model, fe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b3265025-f5b5-476e-bcd6-4defa2aae940",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ImageClassifierOutput' object has no attribute 'last_hidden_state'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m      6\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m----> 8\u001b[0m last_hidden_state \u001b[38;5;241m=\u001b[39m \u001b[43moutputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlast_hidden_state\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(last_hidden_state)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#features = outputs.logits.mean(dim=1).squeeze().numpy()\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ImageClassifierOutput' object has no attribute 'last_hidden_state'"
     ]
    }
   ],
   "source": [
    "image_path= './Soybean_Seeds/Intact_soybeans/1.jpg'\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "inputs = fe(images=image, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "last_hidden_state = outputs.last_hidden_state\n",
    "print(last_hidden_state)\n",
    "#features = outputs.logits.mean(dim=1).squeeze().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c23964b-1656-4a3a-9b4b-29c2393d6d1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
